# Chapter 14

# Service Breadth

## Google Compute Engine (GCE)

This is a zonal service.
And it lets you rent virtual machines on demand.
Now those machines start off with nothing more
than the Operating System on them.
But you can install whatever software you want
and use them however you want.
You could run them as a web server or a database server,
do machine learning on them, run analytics processing.
I mean, you could even play video games on them,
if you really wanted to.

- This is infrastructure as a service,

The cost per hour, or per second if you will,
gets even cheaper if you tell Google
that your instance is **preemptible (**turn it off whenever they need)

## Google Kubernetes Engine (GKE)

How this service works is that you'll create
a GKE cluster in a particular region.
And it will then create GCE instances for you
in the different zones in that region.
At that point, you'll give it
whatever containers you want and some instructions
around them, and it will put those containers
on those different instances automatically for you.
If load on your system goes up,
and you'll need more containers,
well, it can just automatically add more containers for you.
The containers themselves can do
whatever you want them to do, as long as
you can package that up in a Docker container.

**Pricing**

Now the pricing for GKE is actually
a lot simpler than it used to be.
First of all, when you create a cluster,
it'll automatically create GCE instances for you
and you pay for those directly.

## Google App Engine (GAE)

is a platform as a service that'll take your code
and run it in whichever region you've chosen.

It really is a platform for application development.
Natively, App Engine supports the Java, Node, Python,
PHP, and Go programming languages.
But it also has an App Engine Flex mode
that can run any container you give it.
So in that way, it can support pretty much any language.

## Google Cloud Functions

This will run your code in response to an event. And at the time I'm recording this, that code can be written in the Node, Python, Java, or Go programming languages. Cloud Functions are what are known as **Functions as a Service.** And although it's not exactly the same thing, people often refer to this just as **Serverless (**Serverless is **a cloud-native development model that allows developers to build and run applications without having to manage servers**
. There are still servers in serverless, but they are abstracted away from app development.)

- You only pay for however long it runs in hundred millisecond blocks, depending on how much CPU capability and RAM you assigned to that particular function. One differentiator for Cloud Functions is that every function automatically gets an HTTP endpoint.
So you can trigger Cloud Functions without having to
set up a separate API gateway.
- You can also trigger Cloud Functions
based on Google Cloud Storage objects, Pub/Sub messages, and a bunch of other things. Because you're not responsible for this scaling, Google can automatically scale this massively for you. If your system gets a flood of new events, Cloud Functions can just drop your code onto a whole bunch of different machines to handle the load.
Now, Cloud Functions is really flexible. It's particularly good for things where the volume changes a lot, like where you go from having no requests coming in to then potentially having a flood of them all at once, and an example of that might be a chatbot, where you're really never sure how many people are going to be using it at that particular time. But it's also really useful for things like
processing messages from a Pub/Sub queue, or dealing with information coming in from Internet of Things devices.
And it can also be really good for automating parts of your infrastructure.

## Storage

### Local SSD

In GCP a mention of local SSD is talking about
the very fast 375 gigabyte solid state drives
that are physically attached to the different servers. (compute instances)

Now I've marked these as zonal resources.
But really, they're even more specific than that.
They're attached to a single specific
Google Compute Engine instance.
And the main reason you'd want to use these,
is because they offer incredibly high performance,
especially when you stripe across several of them.
Now, it's very important to note
that all data stored on the local SSD drive
will be lost whenever the instance shuts down,
whether that's on purpose or not.
But it is pretty nice to know
that Google does some magic behind the scenes
so that they can actually move your instance
from one place to another called a live migration.
And this data comes along for the ride.

Now, when you start up an instance,
with local SSD attached, you'll start paying
for the local SSDs for however long you keep
that instance running.

### Persistent disk service

Now, this is flexible block-based network-attached storage and it's the boot disk for every Google Compute Engine instance. Now this is block storage, so you format and use them like a physically attached hard drive, but they're not physically attached. Now, their maximum performance is way below the local SSD performance. But it's still fast enough for most users. Up to 40, 000 read IOPS on a single instance. Now the really nice thing about persistent disks is that they persist and they're replicated for durability. Unlike local SSD, these can stick around even after you shut down the instance. And you can reattach them to a new instance in that zone.

Now, persistent disks also allow you to take snapshots
and make machine images out of them. And that adds even more capability and flexibility. Now, these snapshots are what you might call "magical". When you do take a snapshot of a drive like this, you only pay the incremental time and storage cost. But you can use or delete any backup like it was a full backup. If you delete something that's the basis for another incremental, Google will automatically behind the scenes move the data into the next incremental so that nothing is lost.

- block based storage system

## Cloud Filestore

fully managed file based storage system

- when you need to have multiple clients being able to write to the same set of files.

![Untitled](Chapter%2014%20fe08a1bc7a39415c9461b0859d499710/Untitled.png)

![Untitled](Chapter%2014%20fe08a1bc7a39415c9461b0859d499710/Untitled%201.png)

- The key thing here is that the performance is consistent.
- Now basically, how this works is that you provision a Cloud Filestore instance in some zone, and that then becomes available to GCE and GKE through your VPC, your private cloud network connecting everything.
- But I would say that the very primary use case for Cloud Filestore is when you're migrating some application to the cloud, when you're doing a lift and shift. That's because this is a perfect drop-in replacement for what you would have been using in your own data center. So you won't have to rock the boat on your architecture. Now I should call out that although Cloud Filestore does fully managed the file serving part of things, you're kind of on your own for backups, at least for now. That said, you should be able to use whatever other backup system you would have used for your NAS device.
- You don't pay for how much you use, you pay for how much you provision, both in terms of the data storage and the access. Now, I mentioned here that you pay for the provision number of terabytes, because although the price is listed per gigabyte, the minimum capacity that you can provision is one terabyte for the standard tier and two
and a half terabytes for the premium tier,
- Now those numbers are shared across all of the clients that are connected to that particular Cloud Filestore instance. So there's no real scaling to be add here, depending on the load you happen to have.

### Cloud storage

One of the most important services in all of GCP, because it's so often acts as the glue between two other services to enable data to flow through the system. Google Cloud Storage or GCS for short, is an infinitely scalable, fully managed version and highly durable **object store**. Now, **object storage** in this description compares to file storage or block storage. So Cloud Storage stores a bunch of objects in what's called a bucket. And that bucket will live either in a particular region or in a multi-region.

- With multi-regional storage, the data will be stored in two
or more physical locations separated by at least 100 miles,
- Also, you can conditionally update objects based on versions. When you turn versioning on for a Cloud Storage bucket, it'll independently store different versions of the objects, so you can go back to older versions if you need to. But you will pay to store all of those versions while they exist. So you might wanna set up lifecycle transitions to expire them after time.
- 99.999999999% durability
- There are several different classes of cloud storage.
There's **multi-regional**, **regional**, **nearline** and **coldline**.
And you can set up lifecycle transitions so that Google will automatically change them from one class to another,
depending on how old those objects are. **Multi-regional** is great when you have data that needs to be spread out,
either because of the request or because of some redundancy requirements you might have. **Regional** is a little less expensive than multi-regional and offers faster access in a particular location. **Nearline** is made for data
that you expect to access less than once a month
and **cold** **line** is made for data that you expect to access less than once a year. Now, the difference between these different classes is mostly in their cost and availability guarantees.
- you can use the gsutil command line for all of them
- you pay for each operation that you perform against Google Cloud Storage. You also pay for however much data you store over time, depending on which class it's stored as. And if you access data from nearline
and coldline storage, you'll pay a per gigabyte retrieval fee for that as well. And don't forget the network transfer pricing. To get your data out of GCP can cost a significant amount.

## Databases

Now to start off with we should ask ourselves why we would want to use a database service
instead of doing it ourselves. So with persistent disk in local SSD you could definitely install your own database on that. If you installed in a local SSD you get really high performance
but you'd have to be a lot more careful about data durability. And if you installed it on persistent disk then you can take snapshots more easily but it's still a fair amount of work to manage.
So if you want to store some data in the SQL database you should probably take a look at cloud SQL instead.

### Cloud SQL

Cloud SQL is a service for fully-managed
and reliable My SQL and PostgreSQL databases.

supports automatic replication, backup, and failover
and a bunch of other database management things.
Now if you need to scale a cloud SQL instance
you have to do that manually whether you want
to make one cloud SQL instance more powerful
or whether you want to attach replicas.

### Cloud Spanner

first horizontally scalable, strongly consistent, relational database service

- Now it says here's that it's strongly consistent
but actually spanner provides external consistency
which is incidentally stronger than strong consistency.
- what it means for you and meis that we can use cloud spanner like a normal SQL database
without having to give up some of the acid guarantees that were used to just because the database is so huge. And when I say huge I mean huge.
- This is not the kind of database you'd use for your neighborhood barbecue sign up app.
When you provision a cloud spanner instance with a certain number of nodes
and in a certain region or multi region you'll start paying until you shut that instance down.
And you'll also pay for however much storage time you actually use. Now the minimum cost for clouds banner instances is relatively high.

### BigQuery

- a serverless column store data warehouse for analytics using SQL.
- In particular bigquery is serverless and offers amazing scale without having to provision anything up front. So you could be running no queries whatsoever for a while and paying nothing for those queries. Only paying a little bit to store the data and then all of a sudden
you might slam it with a whole bunch of huge queries. No big deal to BiQquery. It scales internally so it can scan terabytes in secondsand petabytes in minutes.
- Now an extra nice thing about BigQuery is that it remembers the queries that you run
and the results from those queries. And if you wind up making the same query again,
against the same data then it'll just give you the result from its cache for free.

### Bigtable

- a low latency and high throughput no SQL database for large operational and analytical applications. This is a wide columnstore type no sequel database
- Cloud BigTable also integrates well with Hadoop and also the cloud dataflow and cloud data proc services
- Cloud BigTable can scale without interrupting its functionality and the storage just scales automatically as you add more data.
- You also pay for however much data you store over time and you can choose to use either cheaper hard drives or faster solid-state drives.

The cloud BigTable is really made huge workloads and if you don't have that level of scale

### Cloud Datastore

a managed and auto scaled no SQL database with indexes queries and acid transaction support. Now just one quick note, these transactions are a little bit different than you might be used to in the SQL world in. That if you change some data within a transaction and then reread that same data you won't see the updated data.

- You pay for however much data you store for however long including the indexes.
- You also pay for the operations that you actually perform so you don't have to pay a certain base amount.

### Firebase realtime database and Cloud fire store

- These are some really interesting no sequel document stores with virtually real-time client updates that happen through managed WebSockets.
- Now firebase DB revolves around a single, potentially huge JSON document that's located in the central US. Cloud fire stores multi-regional and it has collections which have documents and those documents contain data. So it got more structure in its hierarchy. So firebase has a spark free tier, a flame flat tier but for usage-based pricing where you also want to use Google cloud platform you go with their blaze plan.
- Now with the real-time DB you pay more for the data you store each month and the data you download. Now with cloud fire store you pay for the operations you do but you pay a lot less for the storage and transfer.

## Data Transfer

- In particular, lets say that you have a lot of data that you need to put into a Google Cloud storage bucket. Well, the first thing that we are going to look at to handle that situation is called the Data Transfer Appliance.
    - This is a rackable, high-capacity storage server that you use to physically ship data to Google to have them put in Google Cloud Storage.
- But let's say that your data wasn't in your own data center, let's say maybe you want to transfer some data from something like S3. Well, the storage transfer service will copy objects like this for you so that you don't need to set upa machine to do it. The destination for the storage transfer service is always GCS bucket. But the source could be S3, an HTTP or HTTPS endpoint, or another GCS bucket. You can set up storage transfer service jobs as either a one-time thing or scheduled recurring transfers. And the service is free to use but you pay for what it winds up doing.

## External networking

### Google domains

Let's say we start with an end user who wants to visit our web app. They might go to their browser, and type in [michaelwebsite.com](http://michaelwebsite.com/). But for their computer to know what that means it's going to have to do a DNS lookup. And that journey starts at the domain registrar. Google domains is aptly, Google's registrar for domain names.

- scalable, reliable and managed authoritative domain name system service.
- 100% uptime guarantee.
- Google also added support for cloud DNS to have private managed zones. So you can set up DNS that is only accessible from within your VPC.
- manage the DNS records that you set up through either the UI, a command line or API calls
- you pay a fixed fee per zone
- You also do pay an amount for how many lookups are done against those DNS records

### Cloud Load Balancing

- offers high performance scalable traffic distribution.
- integrated with auto-scaling and Cloud CDN.
- highly integrated with Google's global network
    - So when you have an IP address externally that's pointing to Google Cloud Load Balancing, it comes to Google's network anywhere that's closest to it around the world. So the first benefit of this is that there's low latency to connect to the Load Balancer.
- since the Load Balancer is actually built into Google's networking fabric.
Then when the request for a particular Load Balancer hits the edge location
of the Google network, Google makes decisions right there of what to do with that request. So even though this global Load Balancer has one IP address across the world, it'll automatically direct traffic to the region closest to the user, so that that user will get the lowest latency connection possible. And if that region reaches capacity, or fails somehow, then the Load Balancer will just redirect new connections to another region with available capacity. And because this logic is built into Google's networking system as opposed to something that happens on the client machine through something like DNS, this can react really quickly to changes in the user's traffic, network, and the health of systems.
- the way that you pay for Cloud Load Balancing is by making your ingress traffic billable in addition to your egress traffic. Plus effectively an hourly rate per load balancing rule that you set up

### Cloud CDN

- low latency content delivery network based on the HTTPS Cloud Load Balancer, that's integrated with Google compute engine and Google Cloud storage
- provides fast, reliable web and video content delivery with global scale and reach
- You can only have Cloud CDN get its data from within the Google Cloud platform. When you're using an HTTPS Load Balancer, this is super simple to turn on. Just a simple checkbox. So Cloud CDN caches data at at CDN locations all around the world. But when a user requests some data that isn't available at the cache already, you wind up with a cache fill request that results in some network egress charges from your origin back to the point of presence.So once that data has made it from the origin to the point of presence, it'll be cached there as appropriate. And then both the initial request and all subsequent requests will result in an egress charge from the point of presence to the client. And those charges vary by location.
- You also pay a bit for the volume of requests that are made to your Cloud CDN distribution. And if you want to get rid of stuff that's cached at the points of presence before those caches expire naturally, then you'll pay a charge per invalidation request too. Of course, you'll also pay normal usage charges for the origin, but this can be dramatically lower if you have a good caching strategy.

### Ingress vs Egress

Egress means traffic that’s leaving from inside the private network out to the public internet

![Untitled](Chapter%2014%20fe08a1bc7a39415c9461b0859d499710/Untitled%202.png)

Ingress refers to unsolicited traffic sent from an address in public internet to the private network – it is not a response to a request initiated by an inside system

![Untitled](Chapter%2014%20fe08a1bc7a39415c9461b0859d499710/Untitled%203.png)

## Internal networking

- send data around within our system

### Virtual Private Cloud (VPC)

- core of all internal networking in GCP
- global IPv4 unicast sofrtware-defined network

**Two modes:**

- automatic
- custom (if you need more control)
    - you can configure subnets each with their own private IP range
- the VPC network itself is global and each subnet is regional
- free to configure, but you'll pay to use certain services within it
and of course for the network egress charges that we've already talked about.

### Cloud Interconnect

- is the set of options for connecting external networks to Google's network.

### Cloud VPN

- this is an IPsec VPN to connect your VPC via the public internet for low-volume data connections.
- when you're setting up the networking for Cloud VPN, you can set it to either have static routing or dynamic routing. And in general, dynamic routing is preferred so that you can make changes without having to recreate the connection.
- Cloud VPN has a 99.9% availability agreement and you pay for however long it's connected and whatever the normal traffic charges are.

### Dedicated Interconnect

- is a direct physical link between your VPC and your own data center for high-volume data connections.
- you pay a fee per 10 gigabit per second link you set up, plus a relatively smaller fee per VLAN attachment. And then you pay reduced egress rates from the VPC that go through the Dedicated Interconnect.

### Cloud router

- Without the Cloud Router, you have to manage static routes and so changing the IP addresses on either side of the VPN connection
- Cloud Router is free to set up, but like usual, you'll pay for the VPC egress charges.

### CDN Interconnect.

- This offers direct, low-latency connectivity to certain CDN providers with cheaper egress rates. Now, this is for the external content delivery networks,
not Google's Cloud CDN service,

## Machine Learning / AI

### Cloud ML Engine

- massively scalable managed service for training ML models and making predictions
- enables apps to use TensorFlow on datasets of any size
- supports online and batych predictions
- or download models and make predictions anywhere
- Training: Pay per hour depending on chosen cluster capabilities
- Prediction: Pay per provisioned node-hour plus by prediction request volume made

### Cloud Vision API

- classifies images into categories, detects objects/faces and finds and reads printed text
- Pay per image based on detection features requested

### Cloud Speech API

- automatic speech recognition to tuyn spoken word audio files into text
- pre trained ML model for recognizing speech into 110+ languages
- pay per 15 seconds of audio processed

### Cloud Natural Language API

- analyse text for sentiment, intent and content classification and extracts info
- Pre trained ML model for understanding what text means so you can act on it
- content classification, puts each document into one of 700+ predefined categories
- Charged per request of 1000 characters, depending on analysis types requested

### Cloud Translation API

- Translate text among 100+ languages optionally auto detects source language
- pre trained ML model for recognizing and translating semantics not just syntax
- pay per characted processed for translation

### Dialogflow

- build conversational interfaces for websites, mobile apps, messaging IoT devices
- Pre trained ML model and service for acceprting, parsing, lexing input and repsonding
- Enables useful chatbots and other natural user interactions with your custom code

### Cloud Video Intelligence API

- annotates videos in GCS (or directly uploaded) with info about what they contain
- pre trained ML model for video scene analysis and subject identification
- pay per minute of video processed, depending on requested detection modes

### Cloud Job Discovery

- helps carreer sites to improve engagement and conversion
- Pre trained ML model to help job seekers search job posting databases

## Big Data and IoT

![Untitled](Chapter%2014%20fe08a1bc7a39415c9461b0859d499710/Untitled%204.png)

1. Ingest
- Pull raw data in, maybe its streaming data from different devices
1. Store
- after all the data comes in, you need to store it somewhere where you will not lose it
1. Process and analyse
- to transform that raw data into some actionable information
1. Explore and Visualize
- turn the result of the analysis into something that is valuable for your business

### Cloud IoT Core

- fully managed service to connect, manage and ingest data from devices globally
- Device Manager handles device identity, authentication , config and control
- Protocol Bridge publiushes incoming data to Cloud Pub/Sub for processing
- CA signed certificates can be used to verify device ownership on first connect (so we dont just accept data from any device)
- pay per MB of data exchangerd with devices (no per device charge)

### Cloud Pub/Sub (publish and subscribe)

- infinitely scalabe at least once messaging for ingestion, decoupling etc.
- can be used for example for data streaming from different devices
- Use cases for this service include balancing workloads and network clusters,
implementing asynchronous workflows, logging the multiple systems, and data streaming from different devices. It's also been describes as a shock absorber that lets you weather outages and frees you up from having to scale every piece of your system and lockstep. This is one of the most important services in GCP because it's the glue that links everything else together. In the same way that Google Cloud storage can help large blobs flow through a system, Cloud Pub/Sub can help all manner of smaller data flow. The way Cloud Pub/Sub works is that you have a topic, and then a publisher sends a message to that topic, which will then get sent to all of that topic's subscribers. A nice thing about this fan out to multiple subscribers for the same topic is that you can even add subscribers later without affecting any part of the existing system. And so in that way it can actually buy your system some agility or adaptability to change.

![Untitled](Chapter%2014%20fe08a1bc7a39415c9461b0859d499710/Untitled%205.png)

- global by default - you can publish from anywhere in the world and consume from anywhere with consistent latency
- based on GCP HTTPS load balancer
- Messages can be up to 10mb and udelivered ones stored for 7 days
- Push mode delivers to HTTPS endpoints and succeeds on HTTP success status code
- Pull mode delivers messages to requesting cliuents and waits for ACK to delete
- Pay for the data volume

### Cloud Dataprep

- tool to let you visually explore, clean and prepare data for analysis without running servers
- Data wrangling tool for business analysts not for IT pros
- Source data from GCS, BQ or file upload directly (CSV, JSON)
- Automaticallty detects schemas, datatypes or anomalies
- Pay for underlying dataflow job plus management overhead charge

### Cloud Dataproc

- when you have your data and you want to process it
- Batch MapReduce processing via configurable managed Spark and Hadoop clusters
- Handles being told to remove or add nodes even while running jobs
- Integraded with cloud sotrage, BQ, Bigrtable and stackdriver services
- Pay directly for the underlying GCE servers used in the cluster
- Best for moving existing Spark/Hadoop setups to GCP
- If you are building a new data processing pipeline you should rather go with Cloud Dataflow instead

### Cloud Dataflow

- smartly autoscaled and fully managed batch or stream MapReduce like processing
- lets you unify your batch and stream processing and its automatically scaled in some really awesome ways
- autoscales and dynamically redestributes lagging work, mid job to optimize run time
- pay for underlying worker GCE via consolidated charges
    - Pay per second for vCPUs , RAM GBs

### Cloud Datalab

- interactive tool for data exploration, analysis, visualization and machine learning
- uses jupyter notebook

### Cloud Data Studio

- data into dashboards and reports
- big data visualization tool for dashboards and reporting
- like tableau

### Cloud Genomics

- store and process genomes and related experiments
- Query complete genomic information of large research projects in seconds

## Identity & Access (Core Security)

### Roles

- collections of permissions to use or manage GCP resources
    - allow you to perform certain actions
- Primitive roles - often too broad
    - Editor = can change things
    - Viewer = read only
    - Owner = can control access and billing
- Predefined roles - give granular access to specific GCP resources (IAM)
- Custom roles - Project or org level collections you define of granular permissions

### Cloud IAM

- Controls access to GCP resources: authorization, not really authentication
- Member is user, group, domain, service accoiunt or the public (”allUsers”)
    - individual google account, google group, G Suite
    - better to use groups than individual google accounts
    - Service account belongs to application/instance not ndividual end user
- every identity has a unique email address including service accounts
- **Policies** bind members to roles at a hierarchy level: Org, Folder, Project, Resource
    - who can do what at which things
    
    ![Untitled](Chapter%2014%20fe08a1bc7a39415c9461b0859d499710/Untitled%206.png)
    

### Service accounts

- special type of google account that represents an application not an end user
- you should use service accounts rather than user accounts or API keys
- use least privilage
- Use cloud platform managed keys
    - no direct downloading, google manages private keys and rotates them once a day

### Cloud Identity

- Identity as a Service to prrovision and manage users and groups
- Free google accounts for non G suite users tied to a verified domain
- Centrally manage all users in google admin console
- Identities work with other google services (e.g. Google chrome)

### Security key enforcement

- USB or Bluetooth 2-step verification device that prevents phishing

### Resource Manager

- centrally manage and secure organization’s projects with custom folder hierarchy
- Organization resource is root node in hirarchy, you create folders per your business needs

![Untitled](Chapter%2014%20fe08a1bc7a39415c9461b0859d499710/Untitled%207.png)

- recycle bin allows undeleting projects
- define custom IAm roles at organization level
- apply IAM policies at organization, folder or project levels

### Cloud IAP

- Guards apps running on GCP via identity verification, not VPN access
- based on Load Balancer and IAM and only passes auther requests through
- grant access to any IAM identities (groups and service accounts)

### Cloud Audit Logging

- answers: “who did what, where and when?” within GCP projects
- cannot be deleted or changed
- kept for 400 days
- only available for services where Google manages the data access

## Security Management - Monitoring and Response

### Cloud Armor

- edge level protection from DDoS and other attacks on global HTTPs Load Balancer
- only available when using HTTPs Load Balancer
- Manage IPs with CIDR based allow block lists (white list/black list)
- more intelligent rules are coming (SQL injections, geo-based, XSS, custom)

### Security Scanner

- free but limited GAE app vulnerability scanner with very low false positive rate
- Can identify:
    - Cross site scripting (XSS)
    - Flash injection
    - Mixed content (HTTP in HTTPS)
    - outdated / insecure libraries

### Cloud DLP API

- finds and optionally redacts sensitive info in unstructed data streams
- help you minimize what you collect expose or copy to other systems
- 50 + sensitive data detectors (credit card numbers, names, social security numbers, passport numbers, driver licence numbers…)

### Event threat detection

- automaticallty scans your stackdriver logs for suspicious activity
- uses industry leading threat intelligence including google safe browsing
- for example:
    - malware, cryptomining, outgoing DDos attacks, port scanning, brute force SSH

### Cloud Security Commander Center (SCC)

- comprehensive security management and data risk platform for GCP
- Security Information and Event Management (SIEM) Software
- helps you prevent detect and respond to threats from a single pane of glass

## Encryption key management

### Cloud Key Management Service (KMS)

- low latency service to manage and use cryptographic keys
- supports symmetric and asymmetric algorithms
- move secrets out of code (and the like) and into the enviromnent in a secure way
- access to the keys tied with identity and access management service and cloud audit logging to authorize and track key usage
- rotate keys used for new encryption either automatically or on demand

### Cloud Hardware Security Module (HSM)

- Cloud KMS keys managed by FIPS 140-2 level 3 certified HSMs
- device hosts encryption keys and performs cryptographic operations
- enables you to meet compliance that mandates hardware environment
- fully integrated with Cloud KMS
    - Same API, features, IAM Integration

## Operations and Management

### Stackdriver

- family of services for monitoring, logging and diagnosing apps on GCP/AWS
- service integrations add lots of value - among stackdriver and with GCP
- One Stackdrtiver account can track multiple:
    - GCP projects
    - AWS accounts
    - Other resources
- simple usage-based pricing

### Stackdriver monitoring

- gives visibility into performance, uptime, overall health of cloud apps
- includes built in/custom metrics, dashboards, global uptime monitoring and alerts
- follow the trail: Links from alerts to dashboards/charts to logs to traces
- Cross cloud
- alerting policy config includes multi condition rules & resource organization
- alert via email, GCP mobile app, SMS, Slack …

### Stackdriver logging

- store, search, analyze monitor and alert on log data & events
- Create real time metrics from log data then alert or chart them on dashboards
- Send real time log data to BQ for advanced analytics and SQL like querying

### Stackdriver error reporting

- counts, analyses, aggregates and tracks crashes in helpful centralized interface
- smartly aggregates errors into meaningful groups tailored to language/framework

### Stackdriver trace

- tracks and displays call tree & timings across distributed systems to debug performance
- detects app latency shift over time by evaluating performance reports
- generate reports on demand and get daily auto reports per traced app

### Stackdriver Debugger

- grabs program state (callstack, variables, expressions) in live deploys
- you can send logpoints that will fire any time that particular piece of code is hit for the next 24 hours
- or you can set a snapshot point that has more information
- Share debugging sessions with others (just send URL)

### Stackdriver profiler

- see what applications are doing in terms of their CPU and memory usage
- you can profile them to identify which parts of your program are least efficient

### Cloud Deployment Manager

- create/manage resources via declarative templates: “Infrastructure as Code”
- like terraform
- it can automatically parallelize a lot of these things
- templates written in YAML, Python or Jinja2
- create and update deployments both support preview

### Cloud Billing API

- programmatically manage billing for GCP projects and get GCP pricing
- Billing config
    - list billing accounts; get details and associated projects for each
    - enable, disable or change projects billing account

## Development and APIs

### Cloud Source Repositories

- hosted, private git repostiories, with integrations to GCP and other hosted repos
- like github
- natural integration with Stackdriver debugger for live debugging deployed apps

### Cloud Build

- continiously takes source code and builds, tests and deploys it - CI/CD service
- Trigger from Cloud Source Repository or zip in GCS
    - Can trigger from Github via Cloud Source Repositories RepoSync
- Push to GCR & export artifacts to GCS or anywhere your build steps write

### Container Registry

- fast, private docker image storage (based on GCS) with Docker v2 registry API
- when Cloud Build builds a Docker image this is the likely place to put it
- like Docker Hub
- IAM integration simplifies build and deployments within GCP

### Cloud Endpoint

- you may wind up exposing whats running as an API
- handles authorization, monitoring, logging, & API keys for APIs backed by GCP
- Proxy Instances are distributed and hook into Cloud Load Balancer
- Super fast extensible service proxy (ESP) container based on nginx
- pay per call to your API

### Apigee

- full featured & enterprise scale API management platform for whole API lifecycle